Gramene's Solr Search
=====================

Author
------

Ken Youens-Clark <kclark@cshl.edu>
Jan 2014

The data that goes into Solr must be first extracted from the various
databases that make up Gramene (Ensembl, ontology, pathways,
Reactome).  As of build 41, we use one Solr schema to hold all the
search data, so it's a relatively generic schema:

* id
* title
* module
* object
* species
* taxonomy 
* content

Ultimately, we need to create CSV files to upload to Solr.  Here's how
to do that.

Ensembl
-------

To export the Ensembl cores:

  $ cd /usr/local/ensembl-live/eg-web-common/utils
  $ perl search_dump.pl -host cabot -port 3306 -user kclark \
    -pass s3kr1+ -dir /scratch/kyc-tmp/ensembl

This creates an XML file for each species which must then be converted
to flatted, line-oriented, delimited files like so:

  $ cd /usr/local/gramene/gramene-lib/scripts/search
  $ ./convert-ensembl-xml.pl -d /scratch/kyc-tmp/solr \
    /scratch/kyc-tmp/ensembl/*

Because of problems with record separators and line lengths, I chose
to use ASCII-delimited (ASC30 = record separator, ASC31 = field
separator), henceforth referred to as "ADT" (ASCII-delimited text).

Pathways
--------

Data from the Pathways Tools is exported to text files by the pathway
curators to be placed on the FTP site.  There will be multiple files,
e.g., "pathways.dat," "reactions.dat," "enzrxns.dat."  You will use
"gramene-lib/scripts/pathway/format-pathway-data.pl" to parse and
combine these files into a single tab-delimited file to load into a
simple MySQL database with the script
"gramene-lib/scripts/pathway/load-pathway-search.pl."  This
tab-delimited file is also used by the Ensembl Xref loader and should
also be placed on the FTP site as it is a generally useful summary of
the pathway data.  

Here is a sample of the file:

        gene_name: AT1G66030
      enzyme_name: fatty acid (omega-1)-hydroxylase
      reaction_id: RXN-7796
    reaction_name:
               ec: 2.1.1.-
       pathway_id: PWY-5129
     pathway_name: sphingolipid biosynthesis (plants)

From there, you can export it to a ADT file:

  $ cd /usr/local/gramene/gramene-lib/scripts/search
  $ ./export-pathways.pl -d /scratch/kyc-tmp/solr

Reactome
--------

Data from the Plant Reactome project is exported to a text file by the
curators, as well.  So far, it looks like this:

    search_term: 10-formyl-THF
     pathway_id: 1119265
      object_id: 419151

In "gramene-lib/scripts/reactome," you will find a simple bash script
called "load-reactome.sh" to drop/create a "DB" on the specified
"HOST" and load your "DATA."  You should edit these variables at the
beginning of the script, then execute it.

Ontologies
----------

The ontology db is built from the lastest OBO files into a MySQL db.
You can export a CSV file using this:

  $ cd /usr/local/gramene/gramene-lib/scripts/search
  $ ./export-ontologies.pl -d /scratch/kyc-tmp/solr-final
  
As it's not necessary to process the ontology file further, you will
be given instructions for uploading this to Solr.

Prep for Solr
-------------

As mentioned above, ontologies can be directly uploaded to Solr, but
all the other files need to be scanned for ontology terms
species/taxonomy and the ADT transformed to CSV for Solr.  To do this, 

  $ cd /usr/local/gramene/gramene-lib/scripts/search
  $ ./add-solr-fields -d /scratch/kyc-tmp/solr-final \
    /scratch/kyc-tmp/solr/*.adt

This step will also create a shell file for uploading the final CSV
files to Solr.  Simply execute the command you're given.

Module Naming
-------------

The naming of the module is important.  In
"gramene-lib/conf/search.yaml," you will see that some modules have
"reusable_schemas" such as Ensembl.  This allows us to use a single
DBIx::Class module for every Ensembl core/variation/funcgen/etc
database.  Therefore, the module should be named "ensembl" + the
species name as it appears in the
"gramene-ensembl/conf/ensembl.registry" file, e.g.,
"ensembl_oryza_sativa."  For these reusable schemas, the species name
will be used for the "species" facet of the Solr search.  So, if you
are adding a new Reactome module for A. thaliana, you should call it
"reactome_arabidopsis_thaliana" (not "reactome_a_thaliana" or
"reactome_arabidopsis") to match other instances of the species name
(e.g., "ensembl_arabidopsis_thaliana").

Setting link URL
----------------

To indicate the URL template to use for each object, edit
"gramene-web-tools/conf/grm-web.yaml."  The "link" section uses
"module.table" (not "object_type" alias) with optional wildcards to
indicate the URL.  All templates should use "TT:" to indicate a
Template Toolkit template as the URLs often require access to
differing fields.  The template will receive the "object," "module,"
"table," "id" (primary key value), and "doc" (the Solr document), so
you can use just about any known value associated with the object to
construct the URL.

The role of the ontology db
---------------------------

The text of each document added to Solr is scanned for valid ontology
terms, i.e., anything that looks like an ontology term ("TO:000001")
is looked up in the current "ontology" database.  If it's found, then
it's added to the object's "ontology" Solr field (which is
multi-valued).  If a valid "GR_tax" term is found associated with the
object, then it can be used to identify the Solr attribute of
"taxonomy" if the module is not one of the "reusable" schemas where it
is expected to be "module" + "_" + "species_name."  Because of all
this, one should probably rebuild the ontology db from the latest
ontology sources before indexing.

Deleting data
-------------

If you find you have a deprecated module or simply wish to remove
something, you can use "gramene-lib/scripts/search/delete.sh" to issue
a "<delete>" command to Solr.  This is irrevocable, so be careful!

Deploying Solr
--------------

Thus far, the method of deploying Solr is to load it on brie, shut it
down, copy the entire "gramene/solr" directory over to gorgonzola,
shut down Solr there, symlink the new index, restart Solr.  As of
build 40, Solr runs as a single-threaded Jetty app.  Maybe it should
be running in some multi-threaded daemon, e.g., maybe as a Tomcat app.
Ken really needs help with this.

